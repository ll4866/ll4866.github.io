---
title: "Assignment 3"
last_modified_at: 2025-02-19T22:00:00-05:00
categories:
  - Assignments
tags:
  - Working
---
## Introduction
*Written and researched by Lucas Lin.*

This assignment explores how well AI can recognize and classify different qualities in cat images generated by other AIs on CIVITAI. I was curious to see if AI could understand and organize these visuals based on categories like Realistic, Artistic, Surreal, Humorous, and Abstract. The goal wasn’t just to test how accurate these models are but to also think about what it means when machines start interpreting images in creative or conceptual ways.

## 1. Building the Corpus
### 1.1 Corpus Desciption
* **Source:** Generated AI Images from CIVITAI, a site where people share and download AI models.
* **Number of Images:** 99
* **Number of Images with Prompt:** 67
* **Attributes:** Realistic, Artistic, Surreal, Humorous, Abstract (only tagged on images with prompts)
* **Organizaiton:** Everything’s sorted into folders named after those five categories.

### 1.2 Data Gathering
I used the IMAGE DOWNLOADER extension to pull 99 of the latest AI-generated cat images from CIVITAI *(from May 1, 2025)*. The default weekly filter gave me a good slice of what’s trending right now. After that, I went back manually and checked each one for its prompt, writing them down if they existed—super important for what comes next.

### 1.3 Organizing Corpus
To give the analysis some structure, I sorted the images into five main styles: Realistic, Artistic, Surreal, Humorous, and Abstract. I didn’t pick these out of nowhere. I asked KIMI to read all the prompts and suggest category sets a few times, then chose the one that made the most sense. I let KIMI guide the classification because I wanted to lean into the logic of the machine—categorizing more from the prompt’s perspective than my personal interpretation of the images. Here's the gist of each category:

1. **Realistic and Photorealistic**: Focused on cats that look super lifelike—clear textures, natural lighting, that sort of thing.

2. **Artistic and Illustrative**: More expressive images—painterly, stylized, or visually interpretive with less emphasis on realism.

3. **Surreal and Fantasy**: Weird, dreamy, exaggerated—images that feel like they come from an alternate universe.

4. **Humorous and Meme-like**: Light-hearted and playful, sometimes outright absurd. These are designed to entertain.

5. **Abstract and Minimalist**: Simpler, more symbolic compositions. Think shapes, colors, forms—less about representing reality.

Since my focus was on whether the AI can connect the prompt to the final image, it made more sense to sort them by prompt rather than visuals.

## 2 Clustering Exercise (Organe Data Mining)
### 2.1 Different Algorithms Clustering
To begin, I clustered the images without their labels—just to see how the algorithm would naturally group them. This follows what Impett and Offert (2024) talked about: when you're reading a corpus through a neural net, you're also kind of letting the machine read it back to you.

Here are some snapshots of what came out of that:

| Cluster Screenshot | Algorithm |
|-------------------|-----------|
| ![This is Cluster Guess 0](/assets/images/assignment-3/ClusterGuess0.png)| `Inception V3`|
| ![This is Cluster Guess 1](/assets/images/assignment-3/ClusterGuess1.png)| `SqueezeNet`|
| ![This is Cluster Guess 2](/assets/images/assignment-3/ClusterGuess2.png)| `Painters` |
| ![This is Cluster Guess3](/assets/images/assignment-3/ClusterGuess3.png)| `DeepLoc` |

What I noticed:

* `Inception V3` Seemed to pick up on object likeness—e.g., black cats were often grouped together.
* `SqueezeNet` Organized more by thematic cues—narrative elements, context, or tone.themes.
* `DeepLoc` Prioritized light and color distribution. Darker images were clustered toward the top left, lighter ones toward the bottom right.
* `Painters` Focused on type and color palette—essentially a visual art-style clustering.

Each clustering algorithm “read” the images differently. These patterns echoed Impett and Offert’s point: different models carry different cultural biases and logics. It’s not about right or wrong—it’s about recognizing that what a model notices says something about its architecture and training history.

## 3 Classification & Consfusion Matrix
### 3.1 Categorizing Images
The categories used here were the same five described earlier. These were derived from prompt analysis using KIMI, not from visual interpretation.

### 3.2 Predition
I expected the AI to at least partially succeed in classification, especially when prompted attributes were direct. But it was pretty inconsistent. There were some accurate clusters, but also plenty of mismatches.

### 3.4 Result
|**A. Inceptionv3**||
|--|--|
|![This is Hierchy 1](/assets/images/assignment-3/Hierchy0.png)|![This is Issue 1](/assets/images/assignment-3/Issue0.png)|
| ![This is Evidence 1](/assets/images/assignment-3/Evidence0.1.png)| C1. It mostly caught the surreal/fantasy ones but seemed to group by solidity—like the texture or color density. | 
|![This is Evidence 2](/assets/images/assignment-3/Evidence0.2.png)| C2. This had mostly artistic ones, but the grouping felt based on playfulness or expression rather than prompt. | 
|![This is Results 0](/assets/images/assignment-3/Results0.png) | `Inception V3` did a better job recognizing abstract stuff, but really struggled with identifying humor, surreal elements, and artistic styles.|

> `Inceptionv3` was able to tell playful and serious apart, but struggled to match my defined categories.

|**B. SqueezeNet**||
|--|--|
|![This is Hierchy 1](/assets/images/assignment-3/Hierchy1.png)|![This is Issue 1](/assets/images/assignment-3/Issue1.png)|
|![This is Evidence 1.1](/assets/images/assignment-3/Evidence1.2.png)| C1: Captured dramatic lighting well, but didn’t recognize the artificial or staged quality.|
|![This is Evidence 1.2](/assets/images/assignment-3/Evidence1.3.png)|C2: Did well with Artistic—most grouped images had detailed eyes/hair and self-portrait energy.| 
|![This is Evidence 1.3](/assets/images/assignment-3/Evidence1.1.png)|	C3: Grouped anime/manga styles together—though these were actually intended to be Realistic based on prompt. Suggests a different idea of “realism.”| 
|![This is Results 1](/assets/images/assignment-3/Results1.png) | `SqueezeNet` performed better than `Inception V3`, especially in identifying thematic and visual traits across most categories. Still had issues with nuance in artistic categories.|

> This model clustered by shared traits and atmosphere. Not perfect, but closer to my expectations.

|**C. Painters**||
|--|--|
|![This is Hierchy 2](/assets/images/assignment-3/Hierchy2.png)|![This is Issue 2](/assets/images/assignment-3/Issue2.png)|
|![This is Evidence 2.1](/assets/images/assignment-3/Evidence2.1.png)| C1: Capture a Anime-style digital art with realistic details|
|![This is Evidence 2.2](/assets/images/assignment-3/Evidence2.2.png)|C2: Features creative cat like images that are not common sense, plaing around with merging cat with another feature.| 
|![This is Results 2](/assets/images/assignment-3/Results2.png) | Painters does a better job in distinguishing between realistic Photorealistic and surreal fantasy. While the other types it does not do a great job.|

> `Painters` model performs well in identifying anime-style digital art with realistic elements, as well as imaginative, non-literal cat depictions—such as cats creatively merged with other features or environments. This strength is particularly evident in the Surreal and Fantasy category, where Painters excels at capturing the visual essence and conceptual playfulness of the style. However, the model struggles to clearly distinguish between other styles, such as Realistic/Photorealistic and Abstract/Minimalist. This indicates that while Painters is effective at recognizing stylized or fantastical imagery, it needs improvement in parsing the more nuanced or overlapping characteristics of other artistic categories.

|**D. DeepLoc**||
|--|--|
|![This is Hierchy 3](/assets/images/assignment-3/Hierchy3.png)|![This is Issue 3](/assets/images/assignment-3/Issue3.png)|
|![This is Evidence 3.1](/assets/images/assignment-3/Evidence3.1.png)| C1: Capture a Anime-style digital art with realistic details|
|![This is Evidence 3.2](/assets/images/assignment-3/Evidence3.2.png)|C2: Features creative cat like images that are not common sense, plaing around with merging cat with another feature.| 
|![This is Results 3](/assets/images/assignment-3/Results2.png) | Painters does a better job in distinguishing between realistic Photorealistic and surreal fantasy. While the other types it does not do a great job.|

> `DeepLoc` 

### 3.4 Reflection
A few thoughts after examining the results:

- **Performance Varies**: SqueezeNet was more aligned with the prompt-based categories. Inceptionv3 didn’t pick up on thematic or narrative elements as well.
- **Category Boundaries Are Fuzzy**: The prompts themselves sometimes contained hybrid or complex descriptions. AI struggled most when an image embodied more than one attribute.
- **AI Sees Differently**: Some classifications were “wrong” by prompt label but “logical” when considering visual similarity. That’s telling.

Example:

|Image | Prompt|
|---|---|
| ![This is Image 66](/assets/images/assignment-3/IMAGE66.jpeg)**Category:** *Realistic and Photorealistic*| raw analog candid grainy photo 3 cute fluffy kitten/spider hybrids with fluffy spider legs (no paws), ultra high definition, on a cosy room bed, sunny morning light, we can see a human hand like a pov from the bed, one of the hybrids is climbing on the hand while other climb the bed, its a pov from me waking up so we can see the raw form of the body under the blanket, the room seems to be of a goth teem from the 90's|

The image and prompt often feel like two entirely separate entities. One opens a world of interpretation and imagination, while the other imposes a frame or limit. A single description can lead to many different visual outcomes, just as a single image can inspire countless interpretations. Ideally, you’d expect a basic alignment between what a prompt describes and what the image delivers—but that’s not always the case.

For instance, the AI might classify an image with a clearly fantastical prompt as “realistic” because it’s reading the visual style rather than the concept. It focuses on lighting, texture, resolution—things that signal photographic realism—even if the content itself (like a cat with dragon wings) is entirely fictional. The AI reads “realness” as a surface quality, not a thematic one.

This shows a fundamental split in how the AI processes data: when categorizing, the model leans on textual information, using keywords to identify the intended style. But when clustering, the model works visually, grouping based on form, color, and visual similarity—not on what the image is “about.”

This difference reveals a core challenge in machine vision: the gap between semantics (meaning) and aesthetics (appearance). As Arnold and Tilton argue in Distant Viewing, computational methods can analyze large image corpora efficiently, but interpretation still depends heavily on context. Our results echo that—AI can sort images by how they look, but it struggles with how they mean.

Ultimately, while AI models have become skilled at recognizing visual features, they’re still developing an understanding of how visual content and textual intent relate. There’s a gap between prompt and picture, between description and depiction. Until models can bridge that gap—understanding both what is seen and what is meant—their classifications will remain just approximations of human interpretation.

## 4 Multimodal Analysis (CLIP/DV Explorer)
### 4.1 Generate Captions
Use 2DCLIP/DV Explorer to auto-caption images. Compare AI-generated text to my own interpretations.
### 4.2 Explore Language-Image Relationship
Try to decipher how CLIrganize the corpus. Revisit the Impett & Offert quote in this context.

## 5 Overall
### 5.1 Summary
Summarize findings from Parts 2–4.
### 5.2 Key Insights
### 5.3 Takeaway

## Links & Sources & Assets
### Links
### Sources
### Assets

**✅ Working**
<br/>
Due: May 11, 2025 23:59 PM
{: .notice}